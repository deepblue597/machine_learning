{"cells":[{"cell_type":"markdown","metadata":{"id":"cQ7YzabwjTmk"},"source":["## Εργασία 3\n","\n","Καλωσήρθατε στην τρίτη σας εργασία. Η εργασία αυτή έχει σκοπό να σας βοηθήσει να εμπεδώσετε τα σύνολα μοντέλων.\n","\n","Στην εργασία αυτή θα πρέπει να συμπληρώσετε κώδικα Python 3 στα σημεία που αναφέρουν # YOUR CODE HERE. Μην τροποποιείτε τον κώδικα που βρίσκεται εκτός αυτών των περιοχών.\n","\n","Πρωτού παραδόσετε την εργασία σας σιγουρευτείτε ότι ο κώδικας σε όλα τα κελιά τρέχει σωστά. Για το σκοπό αυτό επιλέξτε από το μενού Χρόνος εκτέλεσης (runtime) -> Επανεκίνηση περιόδου λειτουργίας και εκτέλεση όλων.\n","\n","Συμπληρώστε το όνομα (NAME) και το AEM σας παρακάτω:\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3-rBqXXbjyR0"},"outputs":[],"source":["NAME = \"ΚΑΚΑΝΔΡΉΣ ΙΑΣΟΝΑΣ\"\n","AEM = \"10003\""]},{"cell_type":"markdown","metadata":{"id":"egArYhcsTG-T"},"source":["**1** Διαβάστε το διαθέσιμο από την sklearn σύνολο δεδομένων breast cancer, χωρίστε το σε δεδομένα εκπαίδευσης (X_train, y_train) και ελέγχου (X_test, y_test) σε ποσοστό 70%/30% αντίστοιχα με τη συνάρτηση train_test_split (τιμή για random_state βάλτε 0). Το σύνολο αφορά τη διάγνωση καρκίνου του μαστού με βάση μεταβλητές που υπολογίζονται από μια ψηφιοποιημένη εικόνα δείγματος μάζας μαστού που λήφθηκε μέσω αναρρόφησης λεπτής βελόνας (FNA). (2 μονάδες)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"deletable":false,"id":"qgaPtNAmTCX7","nbgrader":{"cell_type":"code","checksum":"29b99b6039fac516d5fa9c7649e40e1d","grade":false,"grade_id":"cell-407a2dea48bfbf80","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# YOUR CODE HERE\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","\n","breast_cancer = load_breast_cancer() \n","X_train , X_test , y_train , y_test = train_test_split( breast_cancer.data ,  breast_cancer.target , test_size= 0.3 , random_state= 0)\n","\n","#raise NotImplementedError()"]},{"cell_type":"code","execution_count":5,"metadata":{"deletable":false,"editable":false,"id":"19LlgZx5cOLP","nbgrader":{"cell_type":"code","checksum":"e4b7add4e469cfc1221bdad01497d404","grade":true,"grade_id":"cell-7387a72f2393ed9a","locked":true,"points":2,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["\"\"\"Τεστ ορθής ανάγνωσης και διαχωρισμού του συνόλου δεδομένων\"\"\"\n","assert round(X_train[0][8], 5) == 0.1779\n","assert round(X_test[0][8], 5) == 0.2116"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["array([[1.149e+01, 1.459e+01, 7.399e+01, ..., 7.431e-02, 2.941e-01,\n","        9.180e-02],\n","       [1.049e+01, 1.861e+01, 6.686e+01, ..., 6.528e-02, 2.213e-01,\n","        7.842e-02],\n","       [1.225e+01, 1.794e+01, 7.827e+01, ..., 8.211e-02, 3.113e-01,\n","        8.132e-02],\n","       ...,\n","       [9.436e+00, 1.832e+01, 5.982e+01, ..., 5.052e-02, 2.454e-01,\n","        8.136e-02],\n","       [9.720e+00, 1.822e+01, 6.073e+01, ..., 0.000e+00, 1.909e-01,\n","        6.559e-02],\n","       [1.151e+01, 2.393e+01, 7.452e+01, ..., 9.653e-02, 2.112e-01,\n","        8.732e-02]])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["X_train"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"data":{"text/plain":["array(30)"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","\n","\n","np.arange(X_train.shape[1])\n","np.array(X_train.shape[1])\n","# np.bincount(np.array([0, 0 , 0, 1, 1, 3, 2, 1, 7, 7 , 7]))\n","# #majority_vote = np.argmax(counts.astype(int))\n","# #majority_vote"]},{"cell_type":"markdown","metadata":{"id":"RB8RexuPciQr"},"source":["**2** Υλοποιήστε μια ντετερμινιστική εκδοχή της μεθόδου των τυχαίων υποχώρων, η οποία χτίζει τόσα μοντέλα όσες και οι μεταβλητές εισόδου, κάθε ένα από τα οποία αγνοεί και μία διαφορετική μεταβλητή εισόδου. Π.χ. το πρώτο μοντέλο αγνοεί την πρώτη, το δεύτερο αγνοεί τη δεύτερη κτλ. Χρησιμοποιήστε τη συνάρτηση clone από το sklearn.base για να δημιουργήστε αντίγραφο του βασικού μοντέλου σε κάθε επανάληψη. (4 μονάδες)\n"]},{"cell_type":"code","execution_count":57,"metadata":{"deletable":false,"id":"KuC_s04KcigR","nbgrader":{"cell_type":"code","checksum":"5fbe3d9c3d8e46ffc9d9cf06a7644fa3","grade":false,"grade_id":"cell-df57dc0d540a2518","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["import numpy as np\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.base import clone\n","\n","class RandomSubspaceDet:\n","    def __init__(self, estimator=DecisionTreeClassifier()):\n","        # YOUR CODE HERE\n","        self.estimator = estimator \n","        self.estimators_ = [] # the list in which will store the estimators\n","        self.features_ = [] # the list in which will store the features that we use for each estimator \n","        \n","        #raise NotImplementedError()\n","\n","    def fit(self, X_train, y_train):\n","        # YOUR CODE HERE\n","        features = X_train.shape[1] # the features of the data\n","        for i in range(features): \n","            estimator = clone(self.estimator) # clone the estimator each time to create a new one \n","\n","            avail_features = np.delete(np.arange(features), i) # get all the columns except the i-th \n","            estimator.fit(X_train[:, avail_features] , y_train) # fit the data with the correct features\n","            \n","            self.estimators_.append(estimator) # add the estimator to the estimators list \n","            self.features_.append(avail_features) # add the features to the feature list \n","\n","\n","\n","        #raise NotImplementedError()\n","\n","    def predict(self, X):\n","        # YOUR CODE HERE\n","        predictions = np.zeros((X.shape[0] , len(self.estimators_))) # create the array that will store the predictions of each estimator \n","        # X.shape[0] -> the samples , len(self.estimators_ -> num of times we make a prediction for each sample ( = num of estimators )\n","        # we bind each estimator with its corresponding features that were used for the training. \n","        for i, (estimator, feature_indices) in enumerate(zip(self.estimators_, self.features_)):\n","            predictions[: , i] = estimator.predict(X[: , feature_indices]) # we use the correct features with the correct estimator to make the predictions \n","            # we store each estimator's predictions to the i-th column \n","        \n","        majority_votes = [] # we will use the majority vote to decide the prediction \n","        for row in predictions: \n","            \n","            counts = np.bincount(row.astype(int)) # find the num of times each prediction appears in all the estimators. We use astype to make the predictions integers\n","            \n","            majority_vote = np.argmax(counts) # store the highest value of prediction \n","            \n","            majority_votes.append(majority_vote)\n","            \n","        #print(majority_votes)\n","        return majority_votes\n","        raise NotImplementedError()"]},{"cell_type":"code","execution_count":58,"metadata":{"deletable":false,"editable":false,"id":"iDNUeGUEciwi","nbgrader":{"cell_type":"code","checksum":"4db933425279be3712175509b4ba2f53","grade":true,"grade_id":"cell-786f87fa5e67b624","locked":true,"points":4,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["\"\"\"Τεστ ορθής υλοποίησης RandomSubspaceDet\"\"\"\n","from sklearn.metrics import accuracy_score\n","\n","rs = RandomSubspaceDet(estimator=DecisionTreeClassifier(random_state=1))\n","rs.fit(X_train, y_train)\n","assert round(accuracy_score(rs.predict(X_test), y_test), 4) == 0.9006"]},{"cell_type":"markdown","metadata":{"id":"n19eEYRNRnG-"},"source":["**3** Υλοποιήστε τη μέθοδο AdaBoost όπως παρουσιάστηκε στο μάθημα. Χρησιμοποιήστε τη συνάρτηση clone από το sklearn.base για να δημιουργήστε αντίγραφο του βασικού μοντέλου σε κάθε επανάληψη. Χρησιμοποιήστε την παράμετρο sample_weight της fit του βασικού μοντέλου για να ορίσετε τα βάρη των παραδειγμάτων εκπαίδευσης. (4 μονάδες)\n"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[{"data":{"text/plain":["398"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.base import clone\n","weights = [] \n","weights = np.full(X_train.shape, 1/len(X_train))\n","len(X_train)"]},{"cell_type":"code","execution_count":186,"metadata":{"deletable":false,"id":"7NOoKPI8TBX6","nbgrader":{"cell_type":"code","checksum":"7fe3d39eaf3d6a53e2b29f9ebd1e7b6a","grade":false,"grade_id":"cell-946d2440bc05714e","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["import numpy as np\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.base import clone\n","\n","class AdaBoost:\n","    def __init__(self, n_estimators=20, estimator=DecisionTreeClassifier(max_depth=1)):\n","        # YOUR CODE HERE\n","        self.n_estimators = n_estimators\n","        self.estimator = estimator \n","        self.weights_  = [] \n","        self.errors = [] \n","        self._estimators = [] \n","        self.log_err = [] \n","        #raise NotImplementedError()\n","\n","    def fit(self, X_train, y_train):\n","        # YOUR CODE HERE\n","        self.weights_ = np.full(len(X_train), 1/len(X_train)) # initialize the weights \n","        #print(self.weights_)\n","        for _ in range(self.n_estimators): \n","            \n","            estimator = clone(self.estimator) # clone the estimator \n","            estimator.fit(X_train , y_train , sample_weight = self.weights_) # fit the data \n","            \n","            y_pred = estimator.predict(X_train) # predict the values of y \n","            \n","            error = np.sum(self.weights_ * (y_pred != y_train)) # sum of the weights where y_pred != y_train \n","            \n","            \n","            log_err = np.log((1 - error) / error)\n","            \n","            self.errors.append(error) \n","            self.log_err.append(log_err)\n","            self._estimators.append(estimator)\n","            \n","            if error == 0 or error >= 0.5 :  # if we have    perfect fit or error > 0.5 we stop \n","                break \n","            \n","            update_value = error / (1 - error) # the update value that needs to be multiplied with correct weights \n","            \n","            \n","            self.weights_ = [self.weights_[i] * update_value if y_pred[i] == y_train[i] else self.weights_[ i] for i in range(len(X_train))  ]\n","            \n","            self.weights_ /= np.sum(self.weights_) # normalize the weights\n","            \n","        #raise NotImplementedError()\n","\n","    def predict(self, X):\n","        # YOUR CODE HERE\n","        \n","        final_pred = np.zeros((X.shape[0] , 2))\n","        \n","        for (estimator, log_err) in zip(self._estimators , self.log_err): \n","            pred = estimator.predict(X) \n","            \n","            for i in range(len(X)):\n","                final_pred[i, pred[i]] += log_err\n","        \n","        pred_result =  np.argmax(final_pred, axis=1)  \n","        \n","        return pred_result\n","        raise NotImplementedError()"]},{"cell_type":"code","execution_count":187,"metadata":{"deletable":false,"editable":false,"id":"jVjqVcv_tmYk","nbgrader":{"cell_type":"code","checksum":"32aff6a2c5ed06a7b34e982fc295d895","grade":true,"grade_id":"cell-88a2903df26757f7","locked":true,"points":4,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["\"\"\"Τεστ ορθής υλοποίησης AdaBoost\"\"\"\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.metrics import accuracy_score\n","\n","ab = AdaBoost(n_estimators=20, estimator=DecisionTreeClassifier(max_depth=1, random_state=1))\n","ab.fit(X_train, y_train)\n","assert round(accuracy_score(ab.predict(X_test), y_test), 4) == 0.9591\n"]},{"cell_type":"code","execution_count":185,"metadata":{"deletable":false,"editable":false,"id":"0QkPMoBNz3T5","nbgrader":{"cell_type":"code","checksum":"71c5a522427b4fc254b65b5d431a767d","grade":false,"grade_id":"cell-4f6f954c3531f480","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# Ίδιο αποτέλεσμα και με τη κλάση της sklearn\n","ab = AdaBoostClassifier(n_estimators=20, algorithm=\"SAMME\", estimator=DecisionTreeClassifier(max_depth=1, random_state=1))\n","ab.fit(X_train, y_train)\n","assert round(accuracy_score(ab.predict(X_test), y_test), 4) == 0.9591"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
